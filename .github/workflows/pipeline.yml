- name: CI/CD Pipeline for MLOps Project

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    env:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Data Preparation
        run: |
          python <<'EOF'
          import pandas as pd
          from sklearn.model_selection import train_test_split
          from huggingface_hub import login, HfApi
          import os

          login(token=os.getenv("HF_TOKEN"), add_to_git_credential=True)
          api = HfApi()

          df = pd.read_csv("tourism.csv")
          for c in ["CustomerID","Unnamed: 0"]:
              if c in df.columns:
                  df.drop(columns=[c], inplace=True)
          df["Gender"] = df["Gender"].replace("Fe Male", "Female")

          for col in ["Age","DurationOfPitch","NumberOfTrips","NumberOfFollowups","MonthlyIncome"]:
              df[col].fillna(df[col].median(), inplace=True)
          for col in ["TypeofContact","Occupation","Gender","ProductPitched","PreferredPropertyStar","MaritalStatus","Designation"]:
              df[col].fillna(df[col].mode()[0], inplace=True)

          X = df.drop("ProdTaken", axis=1)
          y = df["ProdTaken"]
          train_df, test_df = train_test_split(pd.concat([X,y], axis=1), test_size=0.2, random_state=42, stratify=y)

          train_df.to_csv("train.csv", index=False)
          test_df.to_csv("test.csv", index=False)

          api.upload_file("train.csv", "data/train.csv", repo_id="dhani10/tourism-app-dataset", repo_type="dataset")
          api.upload_file("test.csv", "data/test.csv", repo_id="dhani10/tourism-app-dataset", repo_type="dataset")
          EOF

      - name: Run Model Training and Registration
        run: |
          python <<'EOF'
          import pandas as pd, numpy as np, os, joblib
          from sklearn.model_selection import GridSearchCV
          from sklearn.preprocessing import StandardScaler, OneHotEncoder
          from sklearn.compose import ColumnTransformer
          from sklearn.pipeline import Pipeline
          from sklearn.tree import DecisionTreeClassifier
          from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix
          from huggingface_hub import login, HfApi
          from datasets import load_dataset

          login(token=os.getenv("HF_TOKEN"), add_to_git_credential=True)
          api = HfApi()

          dataset = load_dataset("csv", data_files={
              "train": "hf://datasets/dhani10/tourism-app-dataset/data/train.csv",
              "test": "hf://datasets/dhani10/tourism-app-dataset/data/test.csv",
          })
          train_df = dataset["train"].to_pandas()
          test_df = dataset["test"].to_pandas()

          X_train, y_train = train_df.drop("ProdTaken", axis=1), train_df["ProdTaken"].astype(int)
          X_test, y_test   = test_df.drop("ProdTaken", axis=1), test_df["ProdTaken"].astype(int)

          num_cols = X_train.select_dtypes(include=np.number).columns.tolist()
          cat_cols = X_train.select_dtypes(include="object").columns.tolist()
          preproc = ColumnTransformer([
              ("num", StandardScaler(), num_cols),
              ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
          ])

          pipe = Pipeline([("preprocessor", preproc), ("classifier", DecisionTreeClassifier(random_state=42))])
          param_grid = {
              "classifier__max_depth": [3, 5, 7, 10],
              "classifier__min_samples_leaf": [1, 2, 4],
              "classifier__min_samples_split": [2, 5, 10],
              "classifier__criterion": ["gini", "entropy"]
          }
          grid = GridSearchCV(pipe, param_grid, cv=5, scoring="f1", n_jobs=-1)
          grid.fit(X_train, y_train)

          best_model = grid.best_estimator_
          joblib.dump(best_model, "best_model.joblib")
          api.upload_file("best_model.joblib", "model/best_model.joblib",
                          repo_id="dhani10/tourism-model", repo_type="model")
          EOF

        - name: Deploy to Hugging Face Spaces
          env:
            HF_TOKEN: ${{ secrets.HF_TOKEN }}
          run: |
            python deploy_to_hf_space.py \
              --space-id dhani10/tourism-app \
              --sdk docker \
              --hardware cpu-basic \
              --path . \
              --set-secret HF_TOKEN=${HF_TOKEN} \
              --set-var HF_MODEL_REPO=dhani10/tourism-model \
              --set-var MODEL_FILE=model/best_model.joblib
